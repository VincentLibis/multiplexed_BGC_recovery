{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONKAT-seq for multigenomic PAC library mapping\n",
    "\n",
    "#### generates networks of biosynthetic domains and track their physical location in the PAC library\n",
    "#### input : demultiplexed AD/KS reads from 1) genomes 2) plate_pools=megapool 3) well_pools\n",
    "#### output : graphml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys \n",
    "import os\n",
    "from os.path import isfile, join\n",
    "from Bio import SeqIO\n",
    "from Bio import SeqFeature\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from itertools import combinations,combinations_with_replacement\n",
    "from scipy import stats\n",
    "import tempfile\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna\n",
    "import csv\n",
    "import operator\n",
    "import networkx.algorithms.community.centrality\n",
    "import networkx.algorithms.centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(s):\n",
    "    print(s)\n",
    "    \n",
    "def makeFasta(headers,seqs,outputfile):\n",
    "   \n",
    "    headers =  ['>'+x if x[0] != '>' else x for x in headers]\n",
    "    fastaList = [val for pair in zip(headers, seqs) for val in pair]\n",
    "    fastaOutputFile = outputfile\n",
    "    fileHandle = open(fastaOutputFile, 'w')\n",
    "    for item in fastaList:\n",
    "        fileHandle.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "def calc_fisher(a,b,c,d):\n",
    "    oddsratio, pvalue = stats.fisher_exact([[a,b],[c,d]])\n",
    "    return oddsratio,pvalue     \n",
    "\n",
    "def execute(command,screen=False):\n",
    "    msg = ''\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    # Poll process for new output until finished\n",
    "    while True:\n",
    "        nextline = process.stdout.readline()\n",
    "        if nextline == '' and process.poll() is not None:\n",
    "            break\n",
    "        msg = msg + nextline\n",
    "        if screen:\n",
    "            sys.stdout.write(nextline)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    output = process.communicate()[0]\n",
    "    exitCode = process.returncode\n",
    "    if (exitCode == 0):\n",
    "        return msg\n",
    "    else:\n",
    "        print(\"%s --> exitcode: %s --> %s\" % (command, exitCode, output))\n",
    "\n",
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "def is_neighbour(pair):\n",
    "    dif = abs(int(pair[0]%384)-int(pair[1]%384))\n",
    "    if dif in [1,15,16,17]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def toPos(well):\n",
    "    well = (well-1)%384\n",
    "    col = (well / 16) + 1\n",
    "    row = chr((well % 16)+65)\n",
    "    return (row+str(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derep_cat(INPATH,OUTPATH, sample_name, \n",
    "                            remove_files=False,\n",
    "                           threads=40, verbose=False,run=True):\n",
    "    \n",
    "    merged_sorted_output_file = OUTPATH + sample_name + '_SORTED.fna'\n",
    "    \n",
    "    if os.path.isfile(merged_sorted_output_file):\n",
    "            log('File exists --> %s' % merged_sorted_output_file)\n",
    "    else:\n",
    "        merged_output_file = OUTPATH + sample_name + '.fna'   \n",
    "        #make directories for processed files \n",
    "        output_subfolders = ['trim/','derep/']\n",
    "        for dirname in output_subfolders:\n",
    "                ensure_dir(OUTPATH + dirname)\n",
    "\n",
    "        demux_files = [f for f in os.listdir(INPATH) if isfile(join(INPATH, f))]\n",
    "        for i,filename in enumerate(demux_files):\n",
    "            if i % 100 == 0 :\n",
    "                print('%s files processed...' % i)\n",
    "\n",
    "            #trim\n",
    "            input_full_path = INPATH + filename\n",
    "            sufix = os.path.splitext(filename)[1]\n",
    "            output_filenmae = filename.replace(sufix,'.trim' + sufix)\n",
    "            output_full_path = OUTPATH + 'trim/' + output_filenmae\n",
    "            strip_left=\"21\"\n",
    "            truncate=\"200\"\n",
    "            cmd = ('vsearch '\n",
    "                   '--threads %s ' \n",
    "                   '--fastx_filter %s '\n",
    "                   '--fastaout %s '\n",
    "                   '--fastq_stripleft %s '\n",
    "                   '--fastq_trunclen %s '\n",
    "                   % (threads,input_full_path,output_full_path,strip_left,truncate)\n",
    "                  )\n",
    "            if verbose:\n",
    "                print('\\n')\n",
    "                print(cmd)\n",
    "\n",
    "            if run:\n",
    "                execute(cmd,screen=verbose)\n",
    "\n",
    "            #dereplicate\n",
    "            input_file = output_full_path\n",
    "            output_filenmae = output_filenmae.replace(sufix,'.derep' + sufix)\n",
    "            output_full_path = OUTPATH + 'derep/' + output_filenmae\n",
    "\n",
    "            cmd = ('vsearch '\n",
    "                   '--threads %s ' \n",
    "                   '--derep_fulllength %s '\n",
    "                   '--strand plus '\n",
    "                   '--output %s '\n",
    "                   '-sizeout '\n",
    "                   '--fasta_width 0'\n",
    "                   % (threads,input_file,output_full_path)\n",
    "                  )\n",
    "            if verbose:\n",
    "                print('\\n')\n",
    "                print(cmd)\n",
    "            if run:\n",
    "                execute(cmd,screen=verbose)\n",
    "\n",
    "        #merge \n",
    "        cmd = 'cat %s/derep/* > %s' %(OUTPATH,merged_output_file)\n",
    "        if verbose:\n",
    "            print('\\n')\n",
    "            print(cmd)\n",
    "\n",
    "        if run:\n",
    "            execute(cmd,screen=verbose)\n",
    "    \n",
    "        \n",
    "        if remove_files:\n",
    "            for dirname in output_subfolders:\n",
    "                try:\n",
    "                    shutil.rmtree(OUTPATH + dirname)\n",
    "                except:\n",
    "                    print('Unable to remove %s...' % (OUTPATH+dirname))\n",
    "                \n",
    "        \n",
    "        \n",
    "        #sortbylength (note: equal length reads are sorted by number of reads )        \n",
    "        cmd = ('vsearch '\n",
    "               '--threads %s ' \n",
    "               '--sortbylength %s '\n",
    "               '--output %s '\n",
    "               % (threads,merged_output_file,merged_sorted_output_file)\n",
    "              )\n",
    "        \n",
    "        if verbose:\n",
    "            print('\\n')\n",
    "            print(cmd)\n",
    "        if run:\n",
    "            execute(cmd,screen=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_filter_clustering_table(INPATH,OUTPATH,sample_name,min_read_size,\\\n",
    "                                      min_subpools=3,relative_th_factor=0.05,\\\n",
    "                                      host_ref_file=False,host_id=0.95,\\\n",
    "                                      remove_files=False,threads=20,verbose=False,run=True):\n",
    "    \n",
    "    centroids_full_path = INPATH + sample_name + '_OTU.fna'\n",
    "    table_full_path = INPATH + sample_name + '_OTU.txt'\n",
    "    \n",
    "    ensure_dir(OUTPATH)\n",
    "    parsed_table_full_path = OUTPATH + sample_name + '_OTU.csv'\n",
    "\n",
    "\n",
    "    ##########\n",
    "    \n",
    "\n",
    "\n",
    "    log('Parsing clustering information from %s...' % table_full_path )\n",
    "    otu = pd.read_csv(table_full_path ,sep='\\t',index_col=None,header=None)\n",
    "    otu.columns  = ['type','cluster','length','ident','strand','','','align','q','h']\n",
    "\n",
    "    log('Populating table features...')\n",
    "    #Extract cluster sizes from type 'C' in otu table\n",
    "    q = otu[otu['type'] == 'C']['q'].values\n",
    "    clusterSize = otu[otu['type'] == 'C']['length'].values\n",
    "    clusterSizeDict = dict(zip(q,clusterSize))\n",
    "\n",
    "    #Only consider 'Hit' or 'Seed' rows \n",
    "    otu = otu[ (otu.type == 'H') | (otu.type == 'S') ]\n",
    "\n",
    "    seeds = otu[otu.h == '*'].index\n",
    "    otu.loc[seeds,'h'] = otu.loc[seeds,'q']\n",
    "\n",
    "    log('Calculating cluster sizes...')\n",
    "    otu['clusterSize'] = otu['h'].apply(lambda x: clusterSizeDict[x])\n",
    "\n",
    "    otu.loc[:,'seed'] = otu['h'].apply(lambda x: x.split(';')[0])\n",
    "    otu.loc[:,'seed'] = otu.loc[:,'seed']  + ';size=' +  otu.loc[:,'clusterSize'].astype(str) \n",
    "\n",
    "    #remove trailing ';' in 'size= ;'\n",
    "    otu['q'] = otu.q.apply(lambda x: x[:-1] if x[-1] ==';' else x)\n",
    "    otu['h'] = otu.h.apply(lambda x: x[:-1] if x[-1] ==';' else x)\n",
    "\n",
    "    #add sequences from centroids file\n",
    "    centroids_indexs = otu[otu['type'] == 'S'].index\n",
    "    centroids_indexer = SeqIO.index(centroids_full_path, \"fasta\")\n",
    "    otu.loc[centroids_indexs,'seq'] =  otu.loc[centroids_indexs,'seed'].apply(lambda x: str(centroids_indexer[x].seq))\n",
    "\n",
    "    otu.loc[centroids_indexs,'domain'] = sample_name\n",
    "\n",
    "    #otu['well'] = otu['q'].apply(lambda x: int(re.findall('\\d{3}',x)[0]))\n",
    "    otu['well'] = otu['q'].apply(lambda x: x.split('.')[0])\n",
    "    \n",
    "\n",
    "\n",
    "    #merge all cluster members present in a well \n",
    "    otu.loc[:,'readSize']= otu['q'].apply(lambda x: int(re.findall('size=(\\d+)',x)[0]))\n",
    "    frames = []\n",
    "    for index,group in otu.groupby('seed'):\n",
    "        d = group.groupby('well').first().reset_index()\n",
    "#         d['readSize'] = group.groupby('well')['readSize'].agg('sum').values\n",
    "#         frames.append(d)\n",
    "#VL edit: only consider OTU if seed > 3 reads\n",
    "        if d[d.type=='S'].readSize.values>=min_read_size:\n",
    "            d['readSize'] = group.groupby('well')['readSize'].agg('sum').values\n",
    "            frames.append(d)\n",
    "    otu = pd.concat(frames).reset_index()\n",
    "    cols_to_keep = [x for x in otu.columns if ('Unnamed') not in x]\n",
    "    otu = otu[cols_to_keep]\n",
    "    \n",
    "    #Count number of wells apperances for each seed \n",
    "    clusterWellDict = otu.groupby('h').well.nunique().to_dict()\n",
    "    otu['clusterWell'] = otu['h'].apply(lambda x: clusterWellDict[x])\n",
    "\n",
    "    log('%s clusters found (%s OTUs)...' % (otu.h.nunique(),len(otu)))\n",
    "\n",
    "\n",
    "    #min_read_size\n",
    "    log('Dropping clusters with min_read_size < %s from table...' % (min_read_size,))\n",
    "    drop_min_read = otu[otu['readSize'] < min_read_size].index\n",
    "    otu.drop(drop_min_read,inplace=True)\n",
    "    log('%s clusters found (%s OTUs)...' % (otu.h.nunique(),len(otu)))\n",
    "    \n",
    "    \n",
    "    otu.to_csv(parsed_table_full_path)\n",
    "    return otu\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_domain_occurances(filtered_clustering_table,MIN_PAIR_COUNT=3,verb=False): \n",
    "    Nwells = filtered_clustering_table['well'].nunique()\n",
    "\n",
    "    if verb:\n",
    "        print('Calculating pairs statistics...')\n",
    "        print('%s subpools found...' % Nwells)\n",
    "\n",
    "    #create a dic where key is well and values are all the seeds inside the well\n",
    "    wells_dict = {} \n",
    "    for w,gr in filtered_clustering_table.groupby('well')['seed']:\n",
    "        wells_dict[w] = gr.unique()\n",
    "        \n",
    "    clusterWellDict = filtered_clustering_table.groupby('seed').well.nunique().to_dict()\n",
    "\n",
    "    #iterate over all wells and generate all pairwise combinations of seeds \n",
    "    #make a dic where key is the pair tuple the value is the number of co-occurance for this pair across plate \n",
    "    if verb:\n",
    "        print('Counting pairs occurances... ')\n",
    "    pairs_dict = defaultdict(list)\n",
    "\n",
    "    if len(wells_dict.keys()) > Nwells: \n",
    "        print('Number of wells is larger than %s...' % Nwells)\n",
    "        sys.exit()\n",
    "\n",
    "    counter = 0\n",
    "    for counter,well in enumerate(wells_dict.keys()):\n",
    "        if ((counter % 96 == 0) & (verb) ):\n",
    "            print('%s wells processed...' % counter)\n",
    "\n",
    "        for pair in combinations(set(wells_dict[well]),2):\n",
    "                pair = tuple(sorted(pair))\n",
    "                pairs_dict[pair].append(well)\n",
    "\n",
    "    if verb:\n",
    "        print('Current pairs count %s' % len(pairs_dict))\n",
    "\n",
    "    #Iterate over all pairs are remove those which appears together below / above cutoff counts\n",
    "    #Removes most of pairs which appear together only once or twince in the entire plate \n",
    "\n",
    "    if verb:\n",
    "        print('Removing pairs with MIN_PAIR_COUNT < %s' % MIN_PAIR_COUNT)\n",
    "    \n",
    "    l = len(pairs_dict)\n",
    "    for i,k in enumerate(pairs_dict.keys()):\n",
    "        if (len(pairs_dict[k]) < MIN_PAIR_COUNT) :\n",
    "            del pairs_dict[k]\n",
    "    if verb:\n",
    "        print('%s pairs removed...' % (l-len(pairs_dict)))\n",
    "\n",
    "    if verb:\n",
    "        print('Current pairs count %s' % len(pairs_dict))\n",
    "        print('Performing pair-wise Fisher test...')\n",
    "\n",
    "    indexs = pairs_dict.keys()\n",
    "    #Build df to hold all pairs and their binomial scores \n",
    "    df = pd.DataFrame(index=indexs,columns = ['V1','V2','Ov1','Ov2','P0','binom','count'])\n",
    "    df['V1'] = [x[0] for x in pairs_dict.keys()]\n",
    "    df['V2'] = [x[1] for x in pairs_dict.keys()]\n",
    "    df['Ov1'] = df.V1.apply(lambda x: clusterWellDict[x])\n",
    "    df['Ov2'] = df.V2.apply(lambda x: clusterWellDict[x])\n",
    "    df['P0'] = [float(a*b)/(Nwells**2) for a,b in zip(df['Ov1'].values,df['Ov2'].values)] \n",
    "    df['wells'] = pairs_dict.values()\n",
    "\n",
    "    df['count'] = df['wells'].apply(lambda x: len(x))\n",
    "\n",
    "    df['a'] = df['count']\n",
    "    df['b'] = df['Ov1'] - df['count']\n",
    "    df['c'] = df['Ov2'] - df['count']\n",
    "    df['d'] = Nwells- df['count']\n",
    "\n",
    "    df['fisher'] = df.apply(lambda row: calc_fisher(row['a'],row['b'],row['c'],row['d']),axis=1).values\n",
    "    df['pvalue'] = df.fisher.apply(lambda x: x[1] )\n",
    "    df['odds'] = df.fisher.apply(lambda x: x[0] )\n",
    "    df['P']  = df.pvalue.apply(lambda x: -np.log10(x))\n",
    "\n",
    "    if verb:\n",
    "        print('Fisher test done')\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) cluster amplicons into OTUs with usearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KSstack='/nasdata/Vincent/Strep/stacks/KS_reads_SORTED.fna'\n",
    "ADstack='/nasdata/Vincent/Strep/stacks/AD_reads_SORTED.fna'\n",
    "KSmegapool='/nasdata/Vincent/Strep/megapools/KS_reads_SORTED.fna'\n",
    "ADmegapool='/nasdata/Vincent/Strep/megapools/AD_reads_SORTED.fna'\n",
    "KSgenomes='/nasdata/Vincent/Strep/genomes/KS_genomes_reads_SORTED.fna'\n",
    "ADgenomes='/nasdata/Vincent/Strep/genomes/AD_genomes_reads_SORTED.fna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dir='/nasdata/Vincent/Strep/All_amplicons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#KS: concatenate all source of reads, dereplicated and SORTED\n",
    "remove_files=True\n",
    "threads=50\n",
    "run=True\n",
    "verbose=False\n",
    "\n",
    "OUTPATH=Dir\n",
    "sample_name='3KS'\n",
    "merged_output_file = OUTPATH +sample_name+'_SORTED.fna'\n",
    "cmd = 'cat %s %s %s > %s' %(KSstack, KSmegapool, KSgenomes,merged_output_file)\n",
    "\n",
    "execute(cmd,screen=verbose)\n",
    "print 'done'\n",
    "\n",
    "centroids_filename = OUTPATH + sample_name + '_OTU.fna'\n",
    "table_filename = OUTPATH + sample_name + '_OTU.txt'\n",
    "merged_sorted_output_file=OUTPATH +sample_name+'_SORTED.fna'\n",
    "\n",
    "cluster_id=0.95\n",
    "\n",
    "#cluster\n",
    "input_file = merged_sorted_output_file\n",
    "centroids_filename = OUTPATH + sample_name + '_OTU.fna'\n",
    "table_filename = OUTPATH + sample_name + '_OTU.txt'\n",
    "\n",
    "cmd = ('vsearch '\n",
    "       '--threads %s ' \n",
    "       '--cluster_smallmem %s '\n",
    "       '--id %s '\n",
    "       '--iddef 1 '\n",
    "       '--sizein '\n",
    "       '--sizeout '\n",
    "       '--centroids %s '\n",
    "       '--uc %s'\n",
    "       % (threads,input_file,cluster_id,centroids_filename,table_filename)\n",
    "      )\n",
    "\n",
    "if run:\n",
    "    execute(cmd,screen=verbose)\n",
    "    \n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#AD: concatenate all source of reads, dereplicated and SORTED\n",
    "\n",
    "sample_name='3AD'\n",
    "merged_output_file = OUTPATH +sample_name+'_SORTED.fna'\n",
    "cmd = 'cat %s %s %s > %s' %(ADstack, ADmegapool, ADgenomes,merged_output_file)\n",
    "\n",
    "execute(cmd,screen=verbose)\n",
    "print 'done'\n",
    "\n",
    "centroids_filename = OUTPATH + sample_name + '_OTU.fna'\n",
    "table_filename = OUTPATH + sample_name + '_OTU.txt'\n",
    "merged_sorted_output_file=OUTPATH +sample_name+'_SORTED.fna'\n",
    "\n",
    "cluster_id=0.95\n",
    "\n",
    "#cluster\n",
    "input_file = merged_sorted_output_file\n",
    "centroids_filename = OUTPATH + sample_name + '_OTU.fna'\n",
    "table_filename = OUTPATH + sample_name + '_OTU.txt'\n",
    "\n",
    "cmd = ('vsearch '\n",
    "       '--threads %s ' \n",
    "       '--cluster_smallmem %s '\n",
    "       '--id %s '\n",
    "       '--iddef 1 '\n",
    "       '--sizein '\n",
    "       '--sizeout '\n",
    "       '--centroids %s '\n",
    "       '--uc %s'\n",
    "       % (threads,input_file,cluster_id,centroids_filename,table_filename)\n",
    "      )\n",
    "\n",
    "if verbose:\n",
    "    print('\\n')\n",
    "    print(cmd)\n",
    "\n",
    "if run:\n",
    "    execute(cmd,screen=verbose)\n",
    "    \n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) filter OTUs (cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name='3KS'\n",
    "INPUT_PATH = Dir\n",
    "OUTPATH = Dir+ 'clustering/'\n",
    "MIN_READ_SIZE=3\n",
    "\n",
    "parse_and_filter_clustering_table(INPUT_PATH, OUTPATH, sample_name,threads=50,\n",
    "                                                                      min_read_size=MIN_READ_SIZE,\n",
    "                                                                      verbose=True)\n",
    "\n",
    "sample_name='3AD'\n",
    "INPUT_PATH = Dir\n",
    "OUTPATH = Dir+ 'clustering/'\n",
    "MIN_READ_SIZE=3\n",
    "\n",
    "parse_and_filter_clustering_table(INPUT_PATH, OUTPATH, sample_name,threads=50,\n",
    "                                                                      min_read_size=MIN_READ_SIZE,\n",
    "                                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63729\n",
      "56489\n",
      "120218\n"
     ]
    }
   ],
   "source": [
    "OUTPATH = Dir+ 'clustering/'\n",
    "AD_OTU=pd.read_csv(OUTPATH + '3AD_OTU.csv',index_col=0)\n",
    "print len(AD_OTU)\n",
    "KS_OTU=pd.read_csv(OUTPATH + '3KS_OTU.csv',index_col=0)\n",
    "print len(KS_OTU)\n",
    "AD_KS_merged_filtered_clustering_table=pd.concat([AD_OTU, KS_OTU], ignore_index=True)\n",
    "AD_KS_merged_filtered_clustering_table['kind'] = AD_KS_merged_filtered_clustering_table.well.apply(lambda x: x.split('_')[0])\n",
    "print len(AD_KS_merged_filtered_clustering_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each cluster, we remove sequences with low read counts (<5%) relative to the maximum read count of a sequence in the cluster, this is done independently for reads acquired in different MiSeq runs (well-pools / plate-pools / genomes) as the maximum read count observed for each cluster varies in each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_wellOnly=AD_KS_merged_filtered_clustering_table[AD_KS_merged_filtered_clustering_table.kind=='well']\n",
    "otu_megapoolOnly=AD_KS_merged_filtered_clustering_table[AD_KS_merged_filtered_clustering_table.kind=='Megapool']\n",
    "otu_genomesOnly=AD_KS_merged_filtered_clustering_table[AD_KS_merged_filtered_clustering_table.kind=='StrepGenome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otu_wellOnly\n",
      "9973 clusters found (82594 OTUs)...\n",
      "Dropping reads with readSize < max_cluster_readsize*0.05 ...\n",
      "9973 clusters found (47015 OTUs)...\n",
      "\n",
      "otu_megapoolOnly\n",
      "2981 clusters found (34035 OTUs)...\n",
      "Dropping reads with readSize < max_cluster_readsize*0.05 ...\n",
      "2981 clusters found (20590 OTUs)...\n",
      "\n",
      "otu_genomesOnly\n",
      "2102 clusters found (3589 OTUs)...\n",
      "Dropping reads with readSize < max_cluster_readsize*0.05 ...\n",
      "2102 clusters found (2668 OTUs)...\n"
     ]
    }
   ],
   "source": [
    "print 'otu_wellOnly'\n",
    "otu=otu_wellOnly\n",
    "\n",
    "relative_th_factor=0.05\n",
    "#relative_th_factor - size thresholding as function max size in biggest OTU\n",
    "print('%s clusters found (%s OTUs)...' % (otu.h.nunique(),len(otu)))\n",
    "print('Dropping reads with readSize < max_cluster_readsize*%s ...' % relative_th_factor)\n",
    "groups = otu.sort_values('readSize').groupby('seed')\n",
    "d = groups.apply(lambda g: g[ (g['readSize'] > (g['readSize'].max()* relative_th_factor)) | (g['type'] == 'S')])\n",
    "otu = d.drop('seed',axis=1).reset_index().drop('level_1',axis=1)\n",
    "\n",
    "otu_wellOnly=otu\n",
    "\n",
    "print('%s clusters found (%s OTUs)...' % (otu_wellOnly.h.nunique(),len(otu)))\n",
    "\n",
    "print ''\n",
    "print 'otu_megapoolOnly'\n",
    "otu=otu_megapoolOnly\n",
    "\n",
    "relative_th_factor=0.05\n",
    "#relative_th_factor - size thresholding as function max size in biggest OTU\n",
    "print('%s clusters found (%s OTUs)...' % (otu.h.nunique(),len(otu)))\n",
    "print('Dropping reads with readSize < max_cluster_readsize*%s ...' % relative_th_factor)\n",
    "groups = otu.sort_values('readSize').groupby('seed')\n",
    "d = groups.apply(lambda g: g[ (g['readSize'] > (g['readSize'].max()* relative_th_factor)) | (g['type'] == 'S')])\n",
    "otu = d.drop('seed',axis=1).reset_index().drop('level_1',axis=1)\n",
    "\n",
    "otu_megapoolOnly=otu\n",
    "\n",
    "print('%s clusters found (%s OTUs)...' % (otu_megapoolOnly.h.nunique(),len(otu)))\n",
    "print ''\n",
    "print 'otu_genomesOnly'\n",
    "otu=otu_genomesOnly\n",
    "\n",
    "relative_th_factor=0.05\n",
    "#relative_th_factor - size thresholding as function max size in biggest OTU\n",
    "print('%s clusters found (%s OTUs)...' % (otu.h.nunique(),len(otu)))\n",
    "print('Dropping reads with readSize < max_cluster_readsize*%s ...' % relative_th_factor)\n",
    "groups = otu.sort_values('readSize').groupby('seed')\n",
    "d = groups.apply(lambda g: g[ (g['readSize'] > (g['readSize'].max()* relative_th_factor)) | (g['type'] == 'S')])\n",
    "otu = d.drop('seed',axis=1).reset_index().drop('level_1',axis=1)\n",
    "\n",
    "otu_genomesOnly=otu\n",
    "\n",
    "\n",
    "print('%s clusters found (%s OTUs)...' % (otu_genomesOnly.h.nunique(),len(otu)))\n",
    "\n",
    "cleaned_merged_filtered_clustering_table=pd.concat([otu_wellOnly, otu_megapoolOnly,otu_genomesOnly], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) localize positions of OTUs in the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## requires specific list of plates pooled in each stack \n",
    "stackList=[[52, 93, 57, 80, 105, 121, 66, 145, 23, 82, 38, 141, 146, 44, 24, 97, 99, 126, 150, 78, 56, 108, 18, 123, 149], [110, 130, 113, 71, 137, 39, 51, 28, 35, 89, 31, 84, 115, 107, 14, 6, 21, 69, 58, 64, 131, 125, 15, 27, 119], [85, 133, 1, 29, 40, 147, 3, 122, 49, 135, 128, 62, 143, 120, 88, 140, 32, 45, 26, 9, 87, 63, 75, 138, 134], [22, 142, 36, 4, 90, 43, 53, 129, 117, 30, 25, 13, 72, 83, 11, 2, 67, 42, 116, 74, 96, 76, 46, 41, 48], [92, 19, 8, 59, 112, 104, 33, 118, 100, 103, 70, 37, 47, 20, 34, 111, 54, 91, 86, 124, 10, 79, 81, 68, 61], [106, 73, 136, 77, 60, 101, 16, 95, 55, 5, 98, 114, 144, 65, 127, 109, 12, 132, 7, 139, 50, 17, 94, 148, 102]]\n",
    "MPstackDict={}\n",
    "for idx, stack in enumerate(stackList):\n",
    "    for MP in stack:\n",
    "        MPstackDict[MP]=idx+1  #query MP, get corresponding stack\n",
    "        \n",
    "wellstackDict={}\n",
    "for i in range(1,2305):\n",
    "    if i%384>0:\n",
    "        wellstackDict[i]=i/384+1\n",
    "    else:\n",
    "        wellstackDict[i]=i/384\n",
    "        \n",
    "\n",
    "Cols=range(1,25)\n",
    "Rows=[]\n",
    "for letter in range(0,16):\n",
    "    Rows.append(string.ascii_uppercase[letter])\n",
    "\n",
    "\n",
    "def platePosition(wellNb):\n",
    "    icol=(wellNb/16)+min(1,wellNb%16)\n",
    "    irow=Rows[wellNb%16-1]\n",
    "    return str(irow+str(icol))\n",
    "\n",
    "\n",
    "OTUgaranteedLoc={}\n",
    "OTU2choicesLoc={}\n",
    "\n",
    "lost=[]\n",
    "appeared=[]\n",
    "unclearLoc=[]\n",
    "\n",
    "for otu, df_group in cleaned_merged_filtered_clustering_table.groupby('seed'):\n",
    "    flagMP=False\n",
    "    flagwell=False\n",
    "    flagGotcha=False\n",
    "    seq=None\n",
    " \n",
    "    \n",
    "    #create a dataframe with 6 stacks as columns and 2 indexes: MP and wells\n",
    "    OTUlocDF=pd.DataFrame(index=['MP','well'],columns=[1,2,3,4,5,6])\n",
    "    OTUlocDF = OTUlocDF.astype(object)\n",
    "    for row_index, row in df_group.iterrows():\n",
    "        well=int(row.well.split('_')[1])\n",
    "\n",
    "        if type(row.seq) is str:\n",
    "            seq=row.seq\n",
    "        if row.well.startswith('Megapool'):\n",
    "            if well<151:\n",
    "                flagMP=True\n",
    "                try:\n",
    "                    OTUlocDF.at['MP',MPstackDict[well]].append(well)\n",
    "                except:\n",
    "                    OTUlocDF.at['MP',MPstackDict[well]]=[well]\n",
    "        elif row.well.startswith('well'):\n",
    "            flagwell=True\n",
    "            try:\n",
    "                OTUlocDF.at['well',wellstackDict[well]].append(platePosition(well-384*(wellstackDict[well]-1)))\n",
    "            except:\n",
    "                OTUlocDF.at['well',wellstackDict[well]]=[platePosition(well-384*(wellstackDict[well]-1))]\n",
    "   # print OTUlocDF\n",
    "\n",
    "    #find stacks with 1 MP and 1 well:\n",
    "    for i in range(1,7):\n",
    "        try:\n",
    "            if len(OTUlocDF.loc['MP',i])==1:\n",
    "                if len(OTUlocDF.loc['well',i])==1:\n",
    "                    flagGotcha=True\n",
    "                    location=str(OTUlocDF.loc['MP',i][0])+'_'+str(OTUlocDF.loc['well',i][0])\n",
    "                    if otu in OTUgaranteedLoc:\n",
    "                        OTUgaranteedLoc[otu].append(location)\n",
    "                    else:\n",
    "                        OTUgaranteedLoc[otu]=[location]\n",
    "                        OTU2choicesLoc.pop(otu, None)\n",
    "            elif len(OTUlocDF.loc['MP',i])*len(OTUlocDF.loc['well',i])<=4:\n",
    "                flagGotcha=True\n",
    "                possibleLocs=list(itertools.product(OTUlocDF.loc['MP',i], OTUlocDF.loc['well',i]))\n",
    "                possibleLocs=[str(x[0])+'_'+str(x[1]) for x in possibleLocs]\n",
    "                if otu in OTU2choicesLoc:\n",
    "                    OTU2choicesLoc[otu].append(possibleLocs)\n",
    "                elif otu not in OTUgaranteedLoc:\n",
    "                    OTU2choicesLoc[otu]=[possibleLocs]\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    if flagMP and not flagwell:\n",
    "        lost.append(otu)\n",
    "        \n",
    "    elif flagwell and not flagMP:\n",
    "        appeared.append(otu)\n",
    "\n",
    "    elif flagwell and flagMP and not flagGotcha:\n",
    "        unclearLoc.append(otu)\n",
    "\n",
    "print 'lost '+str(len(lost))\n",
    "print 'appeared '+str(len(appeared))\n",
    "print 'unclear '+str(len(unclearLoc))\n",
    "print 'OTUgaranteedLoc '+str(len(OTUgaranteedLoc))\n",
    "print 'OTU2choicesLoc '+str(len(OTU2choicesLoc))\n",
    "\n",
    "locsDict={}\n",
    "for i in OTU2choicesLoc:\n",
    "    locsDict[i]=OTU2choicesLoc[i]\n",
    "for i in OTUgaranteedLoc:\n",
    "    locsDict[i]=OTUgaranteedLoc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedDF=cleaned_merged_filtered_clustering_table[cleaned_merged_filtered_clustering_table.type=='S']\n",
    "\n",
    "#create a dic where key is well and values are all the seeds inside the well\n",
    "wells_dict = {} \n",
    "for w,gr in cleaned_merged_filtered_clustering_table.groupby('seed')['well']:\n",
    "    wells_dict[w] = gr.unique()\n",
    "\n",
    "seedDF['found']= seedDF.seed.apply(lambda x: wells_dict[x])\n",
    "\n",
    "def uniqueGenome(found):\n",
    "    G=[]\n",
    "    for i in found:\n",
    "        if i.startswith('StrepGenome'):\n",
    "            G.append(i)\n",
    "    if len(set(G))==1:\n",
    "        return G[0]\n",
    "    elif len(set(G))==0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return 'multiple'\n",
    "    \n",
    "\n",
    "def genomes(found):\n",
    "    G=[]\n",
    "    for i in found:\n",
    "        if i.startswith('StrepGenome'):\n",
    "            G.append(i)\n",
    "    return len(set(G))\n",
    "\n",
    "def wells(found):\n",
    "    W=[]\n",
    "    for i in found:\n",
    "        if i.startswith('well'):\n",
    "            W.append(i)\n",
    "    return len(set(W))\n",
    "\n",
    "def megapools(found):\n",
    "    M=[]\n",
    "    for i in found:\n",
    "        if i.startswith('Megapool'):\n",
    "            M.append(i)\n",
    "    return len(set(M))\n",
    "\n",
    "def megapoolsList(found):\n",
    "    M=[]\n",
    "    for i in found:\n",
    "        if i.startswith('Megapool'):\n",
    "            M.append(i.split('_')[1])\n",
    "    return str(M)\n",
    "\n",
    "def location(seed):\n",
    "    if seed in locsDict:\n",
    "        return str(locsDict[seed])\n",
    "    elif seed in unclearLoc:\n",
    "        return 'unclear'\n",
    "    elif seed in appeared:\n",
    "        return 'appeared'\n",
    "    else:\n",
    "        return 'lost'\n",
    "\n",
    "seedDF['uniqueGenome']= seedDF.found.apply(lambda x: uniqueGenome(x))\n",
    "seedDF['genomes']= seedDF.found.apply(lambda x: genomes(x))\n",
    "seedDF['wells']= seedDF.found.apply(lambda x: wells(x))\n",
    "seedDF['megapools']= seedDF.found.apply(lambda x: megapools(x))\n",
    "seedDF['megapoolsList']= seedDF.found.apply(lambda x: megapoolsList(x))\n",
    "seedDF['location']= seedDF.seed.apply(lambda x: location(x))\n",
    "seedDF['network']='none'\n",
    "seedDF['network_tag']='none'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) evaluate co-occurrences and connect domains into networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. evaluate co-occurrences between biosynthetic domains in each category of pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating pairs statistics...\n",
      "97 subpools found...\n",
      "Counting pairs occurances... \n",
      "0 wells processed...\n",
      "96 wells processed...\n",
      "Current pairs count 53360\n",
      "Removing pairs with MIN_PAIR_COUNT < 1\n",
      "0 pairs removed...\n",
      "Current pairs count 53360\n",
      "Performing pair-wise Fisher test...\n",
      "Fisher test done\n"
     ]
    }
   ],
   "source": [
    "#1) co-occurrences in well-pools:\n",
    "domain_occurances_table = calc_domain_occurances(otu_wellOnly ,MIN_PAIR_COUNT=1, verb=True)\n",
    "\n",
    "otu_wellOnly.set_index('seed', inplace=True)\n",
    "seedDF.set_index('seed', inplace=True)\n",
    "seedDF['seed']=seedDF.index\n",
    "otu_wellOnly['uniqueGenome']=seedDF['uniqueGenome']\n",
    "otu_wellOnly['location']=seedDF['location']\n",
    "otu_wellOnly['megapoolsList']=seedDF['megapoolsList']\n",
    "otu_wellOnly['seed']=otu_wellOnlyTest.index\n",
    "otu_wellOnly['seq']=seedDF['seq']\n",
    "\n",
    "#2) co-occurrences in genomes:\n",
    "domain_occurances_tableGenome= calc_domain_occurances(otu_genomesOnly ,MIN_PAIR_COUNT=1, verb=True)\n",
    "\n",
    "#3) co-occurrences in plate-pools:\n",
    "domain_occurances_tableMP = calc_domain_occurances(otu_megapoolOnly ,MIN_PAIR_COUNT=1, verb=True)\n",
    "\n",
    "#4) combine all 3 co-occurrences:\n",
    "domain_occurances_table_combined=domain_occurances_table.copy()\n",
    "domain_occurances_table_combined['combined_locations']=domain_occurances_table['wells'].copy()\n",
    "domain_occurances_table_combined['pvalueMP']=1\n",
    "domain_occurances_table_combined['pvalueGenomes']=1\n",
    "\n",
    "idx = domain_occurances_table.index.intersection(domain_occurances_tableMP.index)\n",
    "domain_occurances_table_combined.loc[idx,'pvalueMP']=domain_occurances_tableMP.loc[idx,'pvalue']\n",
    "domain_occurances_table_combined.loc[idx,'combined_locations']=domain_occurances_table_combined.loc[idx,'combined_locations']+domain_occurances_tableMP.loc[idx,'wells']\n",
    "idx = domain_occurances_table.index.intersection(domain_occurances_tableGenome.index)\n",
    "domain_occurances_table_combined.loc[idx,'pvalueGenomes']=domain_occurances_tableGenome.loc[idx,'pvalue']\n",
    "domain_occurances_table_combined['combined_pvalue']=domain_occurances_table_combined['pvalue']*domain_occurances_table_combined['pvalueMP']*domain_occurances_table_combined['pvalueGenomes']\n",
    "\n",
    "def multireps(wells):\n",
    "    score=0\n",
    "    plateLocs=[]   \n",
    "    for w in wells:   \n",
    "        if w.startswith('well'):\n",
    "            plateLocs.append(int(w.split('_')[1]))\n",
    "        elif w.startswith('Strep'):\n",
    "            score+=1\n",
    "        elif w.startswith('Mega'):\n",
    "            score+=1\n",
    "    if max(plateLocs)-min(plateLocs)>384:\n",
    "        score+=1\n",
    "    return score\n",
    "\n",
    "domain_occurances_table_combined['multireplicate'] = domain_occurances_table_combined.apply(lambda x: multireps(x['combined_locations']),axis=1).values\n",
    "domain_occurances_table_final = domain_occurances_table_combined[(domain_occurances_table_combined['multireplicate']>=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. apply threshold on pvalue and generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlibis/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing network --> pvalue 1e-05\n",
      "2805 nodes and 17864 edges found...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlibis/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:40: FutureWarning: 'seed' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2805 nodes\n",
      "17864 edges\n",
      "291 networks\n"
     ]
    }
   ],
   "source": [
    "alpha = 10**-5\n",
    "beta=30\n",
    "\n",
    "method = 'pvalue'\n",
    "networkFile = Dir+ 'Combined_Multigenome_networks_' + str(alpha) + '_' + str(beta)+ '_'  + str(method) +'3.graphml'\n",
    "filtered_clustering_table=otu_wellOnly.copy()\n",
    "G=nx.Graph()\n",
    "pairs_occurances=domain_occurances_table_final[domain_occurances_table_final['odds'] > beta]\n",
    "pairs_occurances['pair'] = zip(pairs_occurances.V1.values,pairs_occurances.V2.values)\n",
    "if method == 'pvalue':\n",
    "    try:\n",
    "        edges = pairs_occurances[pairs_occurances['combined_pvalue'] < alpha]['pair'].apply(lambda x: ast.literal_eval(x)).values\n",
    "    except:\n",
    "        edges = pairs_occurances[pairs_occurances['combined_pvalue'] < alpha]['pair'].values\n",
    "    finally:\n",
    "        risks = pairs_occurances[pairs_occurances['combined_pvalue'] < alpha]['combined_pvalue'].apply(lambda x: -np.log10(x)).astype(str).values\n",
    "        weightedEdges = [ e + (b,) for e,b  in zip(edges,risks)]\n",
    "else:\n",
    "    (reject, pvals_correct,a,b) = multipletests(pairs_occurances.pvalue.values,alpha,method)\n",
    "    pairs_occurances['pvalue_correct'] = pvals_correct\n",
    "    pairs_occurances['reject'] = reject\n",
    "    try:\n",
    "        edges = pairs_occurances[pairs_occurances['reject']]['pair'].apply(lambda x: ast.literal_eval(x)).values\n",
    "    except:\n",
    "        edges = pairs_occurances[pairs_occurances['reject']]['pair'].values\n",
    "    finally:\n",
    "        risks = pairs_occurances[pairs_occurances['reject']]['pvalue_correct'].apply(lambda x: -np.log10(x)).astype(str).values\n",
    "        weightedEdges = [ e + (b,) for e,b  in zip(edges,risks)]\n",
    "\n",
    "G.add_weighted_edges_from(weightedEdges)\n",
    "\n",
    "log(\"Constructing network --> %s %s\" % (method,alpha))\n",
    "log(\"%s nodes and %s edges found...\" % (len(G.nodes()),len(G.edges)))\n",
    "\n",
    "\n",
    "wellsReadsDict = filtered_clustering_table.astype(str).groupby('seed')['well'].apply( lambda x: set(x.tolist())).to_dict()\n",
    "s = pd.Series(wellsReadsDict)\n",
    "attr_dict = s.apply(lambda x: '_'.join(sorted(list(x)))).to_dict()\n",
    "nx.set_node_attributes(G, name='well', values=attr_dict)\n",
    "\n",
    "#extract attributes from filtered_clustering_table to graph\n",
    "for attr in ['seq','clusterSize','domain','uniqueGenome','location','megapoolsList']:\n",
    "    attr_dict = dict(zip(filtered_clustering_table.loc[:,'seed'],filtered_clustering_table.loc[:,attr]))\n",
    "    nx.set_node_attributes(G, name=attr, values=attr_dict)\n",
    "\n",
    "print str(nx.number_of_nodes(G))+\" nodes\"\n",
    "print str(nx.number_of_edges(G))+\" edges\"\n",
    "print str(nx.number_connected_components(G))+\" networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. cleanup graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing ('well_0188.M03834:162:000000000-CYRCK:1:1101:7331:14218;size=64', 'well_1156.M03834:162:000000000-CYRCK:1:1101:14140:2719;size=22789')\n",
      "\n",
      "removing ('well_0188.M03834:162:000000000-CYRCK:1:1101:7331:14218;size=64', 'well_0188.M03834:162:000000000-CYRCK:1:1101:21651:3291;size=9705')\n",
      "\n",
      "removing ('well_0188.M03834:162:000000000-CYRCK:1:1101:7331:14218;size=64', 'well_0188.M03834:162:000000000-CYRCK:1:1101:8205:2362;size=11033')\n",
      "\n",
      "removing ('well_0188.M03834:162:000000000-CYRCK:1:1101:7331:14218;size=64', 'well_0618.M03834:162:000000000-CYRCK:1:1101:5561:9353;size=1243')\n",
      "\n",
      "removing ('well_0397.M03834:162:000000000-CYRCK:1:1103:4966:20365;size=205', 'well_0397.M03834:162:000000000-CYRCK:1:1115:12123:10706;size=36')\n",
      "\n",
      "removing ('well_0296.M03834:162:000000000-CYRCK:1:1101:7351:4761;size=8744', 'well_1484.M03834:162:000000000-CYRCK:1:1101:15472:1958;size=19006')\n",
      "\n",
      "removing ('well_1898.M03834:162:000000000-CYRCK:1:1101:18452:3823;size=3105', 'well_1484.M03834:162:000000000-CYRCK:1:1101:15472:1958;size=19006')\n",
      "\n",
      "removing ('well_1484.M03834:162:000000000-CYRCK:1:1101:15472:1958;size=19006', 'well_2184.M03834:162:000000000-CYRCK:1:1102:8074:7509;size=145')\n",
      "\n",
      "removing ('well_1512.M03834:162:000000000-CYRCK:1:1108:6942:13320;size=38', 'well_0397.M03834:162:000000000-CYRCK:1:1115:12123:10706;size=36')\n",
      "\n",
      "removing ('well_1327.M03834:162:000000000-CYRCK:1:1101:21490:3201;size=1120', 'well_0018.M03834:162:000000000-CYRCK:1:1101:11493:2007;size=840')\n",
      "\n",
      "removing ('well_1254.M03834:162:000000000-CYRCK:1:1101:10481:5103;size=9210', 'well_0018.M03834:162:000000000-CYRCK:1:1101:11493:2007;size=840')\n",
      "\n",
      "removing ('well_0006.M03834:162:000000000-CYRCK:1:1101:15740:4310;size=1312', 'well_0738.M03834:162:000000000-CYRCK:1:1101:22476:2082;size=55600')\n",
      "\n",
      "removing ('well_1327.M03834:162:000000000-CYRCK:1:1101:21490:3201;size=1120', 'well_1167.M03834:162:000000000-CYRCK:1:1101:19816:2019;size=373619')\n",
      "\n",
      "removing ('well_1327.M03834:162:000000000-CYRCK:1:1101:21490:3201;size=1120', 'well_0738.M03834:162:000000000-CYRCK:1:1101:22476:2082;size=55600')\n",
      "\n",
      "removing ('well_0006.M03834:162:000000000-CYRCK:1:1101:15740:4310;size=1312', 'well_1167.M03834:162:000000000-CYRCK:1:1101:19816:2019;size=373619')\n",
      "\n",
      "removing ('well_1327.M03834:162:000000000-CYRCK:1:1101:21490:3201;size=1120', 'well_0738.M03834:162:000000000-CYRCK:1:1101:21024:2660;size=20188')\n",
      "\n",
      "removing ('well_1327.M03834:162:000000000-CYRCK:1:1101:21490:3201;size=1120', 'well_0738.M03834:162:000000000-CYRCK:1:1101:8157:2386;size=8069')\n",
      "\n",
      "removing ('well_0312.M03834:162:000000000-CYRCK:1:1101:10030:19626;size=354', 'well_1909.M03834:162:000000000-CYRCK:1:1101:11264:1899;size=178523')\n",
      "\n",
      "removing ('well_1501.M03834:162:000000000-CYRCK:1:1101:18712:2216;size=117', 'well_1159.M03834:162:000000000-CYRCK:1:1101:12131:2373;size=109754')\n",
      "\n",
      "removing ('well_2298.M03834:162:000000000-CYRCK:1:1101:14514:22831;size=236', 'well_2077.M03834:162:000000000-CYRCK:1:1101:21671:2005;size=6069')\n",
      "\n",
      "removing ('well_0229.M03834:162:000000000-CYRCK:1:1101:17259:7981;size=5202', 'well_2298.M03834:162:000000000-CYRCK:1:1101:14514:22831;size=236')\n",
      "\n",
      "removing ('well_1443.M03834:162:000000000-CYRCK:1:1101:7146:4032;size=6494', 'well_1159.M03834:162:000000000-CYRCK:1:1101:12131:2373;size=109754')\n",
      "\n",
      "removing ('well_0312.M03834:162:000000000-CYRCK:1:1101:10030:19626;size=354', 'well_1501.M03834:162:000000000-CYRCK:1:1101:18712:2216;size=117')\n",
      "\n",
      "removing ('well_2089.M03834:162:000000000-CYRCK:1:1104:15203:11707;size=71', 'well_1528.M03834:162:000000000-CYRCK:1:1101:18689:2725;size=9772')\n",
      "\n",
      "removing ('well_0010.M03834:162:000000000-CYRCK:1:1101:15418:3707;size=5743', 'well_0131.M03834:162:000000000-CYRCK:1:1101:17658:9814;size=397')\n",
      "\n",
      "removing ('well_0010.M03834:162:000000000-CYRCK:1:1101:24265:4379;size=1245', 'well_0010.M03834:162:000000000-CYRCK:1:1101:15418:3707;size=5743')\n",
      "\n",
      "removing ('well_1203.M03834:162:000000000-CYRCK:1:1108:23765:12528;size=85', 'well_0131.M03834:162:000000000-CYRCK:1:1101:17658:9814;size=397')\n",
      "\n",
      "removing ('well_0780.M03834:162:000000000-CYRCK:1:1101:20518:3615;size=1208', 'well_0015.M03834:162:000000000-CYRCK:1:1101:24280:9746;size=2464')\n",
      "\n",
      "removing ('well_0762.M03834:162:000000000-CYRCK:1:1101:14398:12017;size=142', 'well_0851.M03834:162:000000000-CYRCK:1:1105:23118:4184;size=62')\n",
      "\n",
      "removing ('well_0015.M03834:162:000000000-CYRCK:1:1101:24280:9746;size=2464', 'well_0752.M03834:162:000000000-CYRCK:1:1101:22906:2341;size=17484')\n",
      "\n",
      "removing ('well_0762.M03834:162:000000000-CYRCK:1:1101:14398:12017;size=142', 'well_1401.M03834:162:000000000-CYRCK:1:1101:16997:16151;size=963')\n",
      "\n",
      "removing ('well_0762.M03834:162:000000000-CYRCK:1:1101:14398:12017;size=142', 'well_0171.M03834:162:000000000-CYRCK:1:1102:2400:10996;size=141')\n",
      "\n",
      "removing ('well_0171.M03834:162:000000000-CYRCK:1:1101:25716:4038;size=4019', 'well_0762.M03834:162:000000000-CYRCK:1:1101:14398:12017;size=142')\n",
      "\n",
      "removing ('well_0171.M03834:162:000000000-CYRCK:1:1101:25716:4038;size=4019', 'well_0851.M03834:162:000000000-CYRCK:1:1105:23118:4184;size=62')\n",
      "\n",
      "removing ('well_0762.M03834:162:000000000-CYRCK:1:1101:14398:12017;size=142', 'well_1401.M03834:162:000000000-CYRCK:1:1101:6449:3757;size=17153')\n",
      "\n",
      "removing node well_2259.M03834:162:000000000-CYRCK:1:1101:24046:3182;size=562\n",
      "\n",
      "removing node well_2298.M03834:162:000000000-CYRCK:1:1101:14514:22831;size=236\n",
      "\n",
      "removing node well_1402.M03834:162:000000000-CYRCK:1:1102:14603:13279;size=50\n",
      "\n",
      "removing node well_0738.M03834:162:000000000-CYRCK:1:1102:24636:17091;size=3784\n",
      "\n",
      "removing node well_2063.M03834:162:000000000-CYRCK:1:1104:19847:11323;size=386\n",
      "\n",
      "removing node well_1531.M03834:162:000000000-CYRCK:1:1101:22363:3888;size=12261\n",
      "\n",
      "removing node well_1931.M03834:162:000000000-CYRCK:1:1101:10014:14660;size=247\n",
      "\n",
      "removing node well_1539.M03834:162:000000000-CYRCK:1:1101:18346:2077;size=27868\n",
      "\n",
      "removing node well_0460.M03834:162:000000000-CYRCK:1:1101:8886:2370;size=82951\n",
      "\n",
      "removing node well_1569.M03834:162:000000000-CYRCK:1:1101:12618:2516;size=6001\n",
      "\n",
      "removing node well_1851.M03834:162:000000000-CYRCK:1:1101:18229:2030;size=149810\n",
      "\n",
      "removing node well_1156.M03834:162:000000000-CYRCK:1:1101:8776:18545;size=243\n",
      "\n",
      "2787 nodes\n",
      "17557 edges\n",
      "337 networks\n"
     ]
    }
   ],
   "source": [
    "G_clean=G.copy()\n",
    "## clean nodes found in unreasonably high number of wells \n",
    "removeList=[]\n",
    "for nd in G_clean.nodes(): \n",
    "    nbWells=len(G_clean.node[nd]['well'].split('well_'))\n",
    "    if nbWells>100:\n",
    "        removeList.append(nd)\n",
    "G_clean.remove_nodes_from(removeList)  \n",
    "\n",
    "def edgeFragility(Subgraph,Edge): #check if two components are formed when removing an edge   \n",
    "    testGraph=Subgraph.copy()\n",
    "    testGraph.remove_edge(Edge[0],Edge[1])\n",
    "    if nx.number_connected_components(testGraph)>1:\n",
    "        #check that components are made of several nodes\n",
    "        #removing 1 edge can at most create 2 fragments\n",
    "        for fragment in nx.connected_component_subgraphs(testGraph, copy=False): \n",
    "            if fragment.number_of_nodes()<3:\n",
    "                return False          \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def nodeFragility(Subgraph,Node): #check if two components are formed when removing a node\n",
    "    testGraph=Subgraph.copy()\n",
    "    testGraph.remove_node(Node)\n",
    "    if nx.number_connected_components(testGraph)==1:\n",
    "        return False \n",
    "    else:\n",
    "        #check that at least 2 components are made of several nodes\n",
    "        #removing 1 node can create more than 2 fragments\n",
    "        largeChuncks=0\n",
    "        for fragment in nx.connected_component_subgraphs(testGraph, copy=False): \n",
    "            if fragment.number_of_nodes()>2:\n",
    "                largeChuncks+=1\n",
    "        if largeChuncks>1:          \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    \n",
    "##Edges\n",
    "removing=True  \n",
    "while removing:\n",
    "    removing=False \n",
    "    for subgraph in nx.connected_component_subgraphs(G_clean, copy=False):   \n",
    "        if subgraph.number_of_nodes()>10:\n",
    "            centrality=nx.edge_betweenness_centrality(subgraph, k=None, normalized=True, weight=None, seed=None)\n",
    "            #remove most central edges if they connect two distinct communities\n",
    "            for k,v in centrality.items():\n",
    "                if v>0.05:\n",
    "                    if edgeFragility(subgraph,k):\n",
    "                        print 'removing '+str(k)\n",
    "                        print ''\n",
    "                        G_clean.remove_edge(k[0],k[1])\n",
    "                        removing=True  \n",
    "\n",
    "##Nodes\n",
    "\n",
    "\n",
    "removing=True  \n",
    "while removing:\n",
    "    removing=False \n",
    "    removeList=[]\n",
    "    for subgraph in nx.connected_component_subgraphs(G_clean, copy=False): \n",
    "        if subgraph.number_of_nodes()>10:\n",
    "            node_centrality=nx.betweenness_centrality(subgraph, k=None, normalized=True, weight=None, endpoints=False, seed=None)\n",
    "            #nodes sorted by decreasing centrality, check if removing one node breaks the subgraph\n",
    "            sorted_nodes = sorted(node_centrality.items(), key=operator.itemgetter(1),reverse=True)            \n",
    "            for k in sorted_nodes:\n",
    "                if nodeFragility(subgraph,k[0]):\n",
    "                    print 'removing node '+str(k[0])\n",
    "                    print ''\n",
    "                    removeList.append(k[0])\n",
    "                    removing=True\n",
    "    G_clean.remove_nodes_from(removeList)\n",
    "\n",
    "print str(nx.number_of_nodes(G_clean))+\" nodes\"\n",
    "print str(nx.number_of_edges(G_clean))+\" edges\"\n",
    "print str(nx.number_connected_components(G_clean))+\" networks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. give numbers to each nodes and networks and create a fasta file for each network (all translation frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "nx.set_node_attributes(G_clean,'0','subgraphTagNumber')\n",
    "dict_subgraphTagNumber={}\n",
    "lenNetworksDict={}\n",
    "netNumber=0\n",
    "for subgraph in nx.connected_component_subgraphs(G_clean, copy=False):     \n",
    "    netNumber+=1\n",
    "    netName=str(netNumber)  \n",
    "    Dir='/nasdata/Vincent/Strep/All_amplicons/'\n",
    "    subgraphFasta= Dir+'networkFasta_combined/'+netName+'_domains.faa'\n",
    "    tagNumber=0\n",
    "    with open(subgraphFasta,'wb') as outfile:\n",
    "        for n in subgraph.nodes(): \n",
    "            size=nx.number_of_nodes(subgraph)\n",
    "            tagNumber+=1\n",
    "            tagName='>'+netName+\"_size\"+str(size)+'_#'+str(tagNumber)\n",
    "            seedDF.loc[n,'network']=netName\n",
    "            seedDF.loc[n,'network_tag']=tagName\n",
    "\n",
    "            frames=['a','b','c']\n",
    "            for idx,f in enumerate(frames):\n",
    "                \n",
    "                coding_dna=G_clean.node[n]['seq']\n",
    "                coding_dna=coding_dna[idx:]\n",
    "                coding_dna=Seq(coding_dna, generic_dna)\n",
    "                proteinSeq=coding_dna.translate(to_stop=True)\n",
    "                if len(proteinSeq) > 60:\n",
    "                    spamwriter = csv.writer(outfile,delimiter=',')\n",
    "                    spamwriter.writerow([tagName+f])\n",
    "                    spamwriter.writerow([proteinSeq])\n",
    "\n",
    "            #update graphml file with domain names and numbers\n",
    "            dict_subgraphTagNumber[n]=tagName\n",
    "            lenNetworksDict[netName]=tagNumber\n",
    "\n",
    "nx.set_node_attributes(G_clean, name='subgraphTagNumber',values=dict_subgraphTagNumber)\n",
    "\n",
    "#concatenate all fasta files into .fna\n",
    "os.chdir(Dir+'networkFasta_combined/')\n",
    "subprocess.call(\"cat *.faa > multigenomeNets.fna\", shell=True)\n",
    "\n",
    "\n",
    "nx.write_graphml(G_clean, networkFile)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Annotate nodes with pblast (identity to known BGCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Extract protein sequences from reference BGCs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mibigFolder='/nasdata/Vincent/Strep/mibig_gbk_2.0/'\n",
    "\n",
    "def extractProteins(genbankFile):\n",
    "    with open(genbankFile, \"rU\") as input_handle:\n",
    "        with open(genbankFile.replace('.gbk','.faa') , \"w\") as output_handle:\n",
    "            CDScounter=0\n",
    "            sequences = SeqIO.parse(input_handle, \"genbank\") \n",
    "            for record in sequences:\n",
    "                record.id=record.description.split(' ')[0]+\"_\"+record.id\n",
    "                record.description=record.id\n",
    "                for seq_feature in record.features :\n",
    "                    if seq_feature.type==\"CDS\" :\n",
    "                        if len(seq_feature.qualifiers['translation'][0])>180:\n",
    "                            CDScounter+=1\n",
    "                            output_handle.write(\">%s_%s\\n%s\\n\" % (\n",
    "                                   record.id,\n",
    "                                   str(CDScounter),\n",
    "                                   seq_feature.qualifiers['translation'][0]))\n",
    "                            \n",
    "if not os.path.exists(mibigFolder+'MIBiG.fnaa'):               \n",
    "    for filename in os.listdir(mibigFolder):\n",
    "        if filename.endswith(\".gbk\"):\n",
    "            if not os.path.exists(mibigFolder+filename.replace('.gbk','.faa')):\n",
    "                extractProteins(mibigFolder+filename)\n",
    "    \n",
    "    os.chdir(mibigFolder)\n",
    "    subprocess.call(\"cat *.faa > MIBiG.fnaa\", shell=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. blastp all translated domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancedir='/nasdata/Vincent/Strep/Distance/'\n",
    "\n",
    "def checkBlast(distancedir,RESULT_FILE,QUERY_FILE,DB_INFILE,DB_OUTFILE):\n",
    "    if not os.path.exists(DB_OUTFILE+'.pin'):\n",
    "        subprocess.call([\"makeblastdb -in \"+DB_INFILE+\" -dbtype prot -out \"+DB_OUTFILE], shell= True)    \n",
    "    if os.path.exists(distancedir+'blastWorks/'+RESULT_FILE):\n",
    "        print \"already blasted \"+RESULT_FILE[:-4]\n",
    "    else:\n",
    "        print '-Blasting '+RESULT_FILE[:-4]+'...'\n",
    "        subprocess.call([\"blastp -query \"+QUERY_FILE+\" -db \"+DB_OUTFILE+\" -outfmt '10 qseqid sseqid evalue bitscore length pident sstart send' -out blastWorks/\"+RESULT_FILE+\" -max_target_seqs 2000000 -evalue 1e-20 -qcov_hsp_perc 80 -num_threads 56\"], shell=True)\n",
    "\n",
    "os.chdir(distancedir)\n",
    "\n",
    "QUERY_FILE='/nasdata/Vincent/Strep/All_amplicons/networkFasta_combined/multigenomeNets.fna' \n",
    "\n",
    "#blast soil against MIBiG    \n",
    "DB_OUTFILE=distancedir+'blastWorks/blastDBs/MIBiG_DB'\n",
    "DB_INFILE=mibigFolder+'MIBiG.fnaa'\n",
    "RESULT_FILE='multigenomes-VS-MIBiGDB.csv'\n",
    "checkBlast(distancedir,RESULT_FILE,QUERY_FILE,DB_INFILE,DB_OUTFILE)\n",
    "\n",
    "print ''\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. read blast results and annotate graph file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multigenomes-VS-MIBiGDB.csv: 1535115 blast hits\n",
      "1970\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlibis/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/vlibis/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "blastFile='/nasdata/Vincent/Strep/Distance/blastWorks/multigenomes-VS-MIBiGDB.csv'\n",
    "blast = pd.read_csv(blastFile,index_col=None,header=None)\n",
    "\n",
    "blast.columns = ['qseqid', 'sseqid', 'evalue', 'bitscore', 'length', 'pident', 'sstart', 'send']\n",
    "blast.sort_values(by='bitscore',ascending=False, inplace=True)  #sort by best Bitscore\n",
    "blast.reset_index(drop=True, inplace=True)   \n",
    "print str(blastFile.split('/')[-1])+': '+str(len(blast))+\" blast hits\"\n",
    "\n",
    "seedDF.loc[:,'AA_id']=74\n",
    "seedDF.loc[:,'top_MIBIG_protein']='None'\n",
    "\n",
    "seedDF.set_index('network_tag', inplace=True)\n",
    "\n",
    "print len(blast.groupby('qseqid'))\n",
    "n=0\n",
    "for otu,group in blast.groupby('qseqid'):\n",
    "    bestHit=('','',0)\n",
    "    n+=1\n",
    "    if n%100==0:\n",
    "        print n\n",
    "    for row_index, row in group.iterrows():  \n",
    "        if bestHit[2]==0:\n",
    "            bestHit=(row.sseqid,row.sstart,row.pident)\n",
    "    if bestHit[2]>74:  \n",
    "        seedDF.loc[str(\">\"+otu[:-1]),'AA_id']=bestHit[2]\n",
    "        seedDF.loc[str(\">\"+otu[:-1]),'top_MIBIG_protein']=bestHit[0]+'_'+str(bestHit[1])\n",
    "\n",
    "seedDF['network_tag']=seedDF.index\n",
    "seedDF.set_index('seed', inplace=True)\n",
    "seedDF['seed']=seedDF.index\n",
    "\n",
    "#extract attributes from filtered_clustering_table to graph\n",
    "for attr in ['AA_id','top_MIBIG_protein']:\n",
    "    attr_dict = dict(zip(seedDF.loc[:,'seed'],seedDF.loc[:,attr]))\n",
    "    nx.set_node_attributes(G_clean, name=attr, values=attr_dict)\n",
    "networkFileOut=Dir+\"Combined_Multigenome_networks_annotated_recoveredOCTOBER.graphml\"\n",
    "nx.write_graphml(G_clean, networkFileOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
